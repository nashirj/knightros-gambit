{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference #\n",
    "\n",
    "#https://web.stanford.edu/~surag/posts/alphazero.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mcts:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.c_punct = 0.5 # exploration rate, figure out a good value for this\n",
    "        self.visited = [] # states that have already been visited\n",
    "        self.Q = {} # figure out data structure for Q and N\n",
    "        self.N = {}\n",
    "        self.P = {}\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    Search refers to each simulation of MCTS\n",
    "    \"\"\"\n",
    "        \n",
    "    def search(self,s,game,nnet): \n",
    "\n",
    "        if game.gameEnded(s): return  -game.gameReward(s) # returns the negative of the game reward, why negative?\n",
    "\n",
    "        if s not in self.visited # if a state has not been visited then you must find the predictions made by the model\n",
    "            self.visited.add(s) # will mark as visisted\n",
    "            self.P[s],v = nnet.predict(s) # this is like the rollout phase but using the nueral network to build policy vector\n",
    "            return -v # still don't understand why negative?\n",
    "        \n",
    "        max_u, best_a = -float(\"inf\"), -1  \n",
    "\n",
    "        for a in game.getvalidActions(s): # finds the valid actions from current state, we will be using pyChess or stockfish for this\n",
    "\n",
    "            u = self.Q[s][a] + self.c_puct*self.P[s][a]*sqrt(sum(self.N[s]))/(1+self.N[s][a]) # formula for evaluating action (we can come up with an evaluation function)\n",
    "\n",
    "            if u>max_u:\n",
    "                max_u = u  # assigns best u\n",
    "                best_a = a  # assigns best a\n",
    "        \n",
    "            a = best_a \n",
    "            sp = game.nextState(s,a) # want to take the the best action\n",
    "            v = self.search(sp,game,nnet) # performs a search on the best action and this will update the values accordingly\n",
    "\n",
    "            self.Q[s][a] = (self.N[s][a] * self.Q[s][a] +v)/self.N[s][a]+1 # calculates Q\n",
    "            self.N[s][a] += 1  # adds 1 to represent the node was visited\n",
    "            \n",
    "            return -v # why negative???\n",
    "\n",
    "    def policy(self,s):\n",
    "\n",
    "         # I believe this should build a policy vector based on the state\n",
    "         # This example considers the policy vecotr as a Normalized Count of N, \n",
    "\n",
    "         return self.N[s] # this is not the actual return, but showing htat it has to do with N     \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \"\"\"\"\n",
    "    1. Initialize nn with random weights, starting with a random policy and value network\n",
    "    2. Play a number of games of self play\n",
    "    3. In each turn of the game perform a fixed number of MCTS simulations from the current state\n",
    "    4. Pick a move by sampling the improved policy \n",
    "    \"\"\"\"\n",
    "\n",
    "class Train:\n",
    "\n",
    "    def __init__(self,numIters,numEps,threshold,numMCTSsims):\n",
    "        self.numIters = numIters\n",
    "        self.numEps = numEps\n",
    "        self.threshold = threshold\n",
    "        self.numMCTSsim = numMCTSsims\n",
    "\n",
    "    def policyIterSp(self,game):\n",
    "\n",
    "        nnet = initNet() # initializes nueral network, we need to work on creating this\n",
    "        trainingData = []\n",
    "        for i in range(self.numIters):\n",
    "            for e in range(self.numEps):\n",
    "                trainingData += self.executeEpisode(game,nnet) # recieves training trainingData\n",
    "            new_nnet = self.trainNNet(trainingData) # trains new nnet on new training trainingData\n",
    "            frac_win = self.pit(new_nnet,nnet) # play the two nueral networks against each other\n",
    "            if frac_win > self.threshold: \n",
    "                nnet = new_nnet # pick the winning model\n",
    "        return nnet\n",
    "\n",
    "\n",
    "    def executeEpisde(self,game,nnet):\n",
    "\n",
    "        trainingData = []\n",
    "        s = game.startState() # get the start state of the game\n",
    "       \n",
    "        mcts = Mcts() # instantiate the MCTS class\n",
    " \n",
    "        while True: \n",
    "\n",
    "            for _ in range(self.numMCTSsims):\n",
    "                mcts.search(s,game,nnet) #performs numMCTSsims monte carlo simulations\n",
    "\n",
    "            trainingData.append([s,mcts.policy(s),None]) # append the state, and improved policy, None refers to not knowing the values yet\n",
    "\n",
    "            a = choice(len(mcts.pi(s)), p=mcts.policy(s)) # choose a random move from the improved policy\n",
    "            s= game.nextState(s,a) # try that random move\n",
    "\n",
    "            if game.gameEnded(s): # if the game is over then you need to assign rewards to all the trainingData\n",
    "                trainingData = self.assignRewards(trainingData,game.gameReward(s))\n",
    "                return trainingData\n",
    "\n",
    "    def assignRewards(self,trainingData,reward):\n",
    "\n",
    "        for i in trainingData:\n",
    "            # assign to each training example here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

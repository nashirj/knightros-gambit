{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e39999f",
   "metadata": {},
   "source": [
    "# Demo speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdef6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1828da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2a4.wav  b3d7.wav  requirements.txt  speech_recognition.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "017b6464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_audio(filename):\n",
    "    with sr.AudioFile(filename) as source:\n",
    "\n",
    "        audio = r.listen(source)\n",
    "        try:\n",
    "            # for testing purposes, we're just using the default API key\n",
    "            # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\")`\n",
    "            # instead of `r.recognize_google(audio)`\n",
    "            print(\"Google Speech Recognition thinks you said:\\n\\t\" + r.recognize_google(audio))\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c45a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize recognizer class (for recognizing the speech)\n",
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a23e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Speech Recognition thinks you said:\n",
      "\t8284\n"
     ]
    }
   ],
   "source": [
    "recognize_audio('a2a4.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efe0553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Speech Recognition thinks you said:\n",
      "\tB3 D7\n"
     ]
    }
   ],
   "source": [
    "recognize_audio('b3d7.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff8b47e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talk\n",
      "Processing...\n",
      "Text: I'm testing my speech recognition to see if it'll cut me off\n"
     ]
    }
   ],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"Talk\")\n",
    "    audio_text = r.listen(source)\n",
    "    print(\"Processing...\")\n",
    "    \n",
    "    try:\n",
    "        # using google speech recognition\n",
    "        print(\"Text: \"+r.recognize_google(audio_text))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Sorry, I did not get that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "872e85a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Recognizer in module speech_recognition object:\n",
      "\n",
      "class Recognizer(AudioSource)\n",
      " |  Method resolution order:\n",
      " |      Recognizer\n",
      " |      AudioSource\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Creates a new ``Recognizer`` instance, which represents a collection of speech recognition functionality.\n",
      " |  \n",
      " |  adjust_for_ambient_noise(self, source, duration=1)\n",
      " |      Adjusts the energy threshold dynamically using audio from ``source`` (an ``AudioSource`` instance) to account for ambient noise.\n",
      " |      \n",
      " |      Intended to calibrate the energy threshold with the ambient energy level. Should be used on periods of audio without speech - will stop early if any speech is detected.\n",
      " |      \n",
      " |      The ``duration`` parameter is the maximum number of seconds that it will dynamically adjust the threshold for before returning. This value should be at least 0.5 in order to get a representative sample of the ambient noise.\n",
      " |  \n",
      " |  listen(self, source, timeout=None, phrase_time_limit=None, snowboy_configuration=None)\n",
      " |      Records a single phrase from ``source`` (an ``AudioSource`` instance) into an ``AudioData`` instance, which it returns.\n",
      " |      \n",
      " |      This is done by waiting until the audio has an energy above ``recognizer_instance.energy_threshold`` (the user has started speaking), and then recording until it encounters ``recognizer_instance.pause_threshold`` seconds of non-speaking or there is no more audio input. The ending silence is not included.\n",
      " |      \n",
      " |      The ``timeout`` parameter is the maximum number of seconds that this will wait for a phrase to start before giving up and throwing an ``speech_recognition.WaitTimeoutError`` exception. If ``timeout`` is ``None``, there will be no wait timeout.\n",
      " |      \n",
      " |      The ``phrase_time_limit`` parameter is the maximum number of seconds that this will allow a phrase to continue before stopping and returning the part of the phrase processed before the time limit was reached. The resulting audio will be the phrase cut off at the time limit. If ``phrase_timeout`` is ``None``, there will be no phrase time limit.\n",
      " |      \n",
      " |      The ``snowboy_configuration`` parameter allows integration with `Snowboy <https://snowboy.kitt.ai/>`__, an offline, high-accuracy, power-efficient hotword recognition engine. When used, this function will pause until Snowboy detects a hotword, after which it will unpause. This parameter should either be ``None`` to turn off Snowboy support, or a tuple of the form ``(SNOWBOY_LOCATION, LIST_OF_HOT_WORD_FILES)``, where ``SNOWBOY_LOCATION`` is the path to the Snowboy root directory, and ``LIST_OF_HOT_WORD_FILES`` is a list of paths to Snowboy hotword configuration files (`*.pmdl` or `*.umdl` format).\n",
      " |      \n",
      " |      This operation will always complete within ``timeout + phrase_timeout`` seconds if both are numbers, either by returning the audio data, or by raising a ``speech_recognition.WaitTimeoutError`` exception.\n",
      " |  \n",
      " |  listen_in_background(self, source, callback, phrase_time_limit=None)\n",
      " |      Spawns a thread to repeatedly record phrases from ``source`` (an ``AudioSource`` instance) into an ``AudioData`` instance and call ``callback`` with that ``AudioData`` instance as soon as each phrase are detected.\n",
      " |      \n",
      " |      Returns a function object that, when called, requests that the background listener thread stop. The background thread is a daemon and will not stop the program from exiting if there are no other non-daemon threads. The function accepts one parameter, ``wait_for_stop``: if truthy, the function will wait for the background listener to stop before returning, otherwise it will return immediately and the background listener thread might still be running for a second or two afterwards. Additionally, if you are using a truthy value for ``wait_for_stop``, you must call the function from the same thread you originally called ``listen_in_background`` from.\n",
      " |      \n",
      " |      Phrase recognition uses the exact same mechanism as ``recognizer_instance.listen(source)``. The ``phrase_time_limit`` parameter works in the same way as the ``phrase_time_limit`` parameter for ``recognizer_instance.listen(source)``, as well.\n",
      " |      \n",
      " |      The ``callback`` parameter is a function that should accept two parameters - the ``recognizer_instance``, and an ``AudioData`` instance representing the captured audio. Note that ``callback`` function will be called from a non-main thread.\n",
      " |  \n",
      " |  recognize_bing(self, audio_data, key, language='en-US', show_all=False)\n",
      " |      Performs speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Microsoft Bing Speech API.\n",
      " |      \n",
      " |      The Microsoft Bing Speech API key is specified by ``key``. Unfortunately, these are not available without `signing up for an account <https://azure.microsoft.com/en-ca/pricing/details/cognitive-services/speech-api/>`__ with Microsoft Azure.\n",
      " |      \n",
      " |      To get the API key, go to the `Microsoft Azure Portal Resources <https://portal.azure.com/>`__ page, go to \"All Resources\" > \"Add\" > \"See All\" > Search \"Bing Speech API > \"Create\", and fill in the form to make a \"Bing Speech API\" resource. On the resulting page (which is also accessible from the \"All Resources\" page in the Azure Portal), go to the \"Show Access Keys\" page, which will have two API keys, either of which can be used for the `key` parameter. Microsoft Bing Speech API keys are 32-character lowercase hexadecimal strings.\n",
      " |      \n",
      " |      The recognition language is determined by ``language``, a BCP-47 language tag like ``\"en-US\"`` (US English) or ``\"fr-FR\"`` (International French), defaulting to US English. A list of supported language values can be found in the `API documentation <https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/bingvoicerecognition#recognition-language>`__ under \"Interactive and dictation mode\".\n",
      " |      \n",
      " |      Returns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the `raw API response <https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/bingvoicerecognition#sample-responses>`__ as a JSON dictionary.\n",
      " |      \n",
      " |      Raises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n",
      " |  \n",
      " |  recognize_google(self, audio_data, key=None, language='en-US', show_all=False)\n",
      " |      Performs speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Google Speech Recognition API.\n",
      " |      \n",
      " |      The Google Speech Recognition API key is specified by ``key``. If not specified, it uses a generic key that works out of the box. This should generally be used for personal or testing purposes only, as it **may be revoked by Google at any time**.\n",
      " |      \n",
      " |      To obtain your own API key, simply following the steps on the `API Keys <http://www.chromium.org/developers/how-tos/api-keys>`__ page at the Chromium Developers site. In the Google Developers Console, Google Speech Recognition is listed as \"Speech API\".\n",
      " |      \n",
      " |      The recognition language is determined by ``language``, an RFC5646 language tag like ``\"en-US\"`` (US English) or ``\"fr-FR\"`` (International French), defaulting to US English. A list of supported language tags can be found in this `StackOverflow answer <http://stackoverflow.com/a/14302134>`__.\n",
      " |      \n",
      " |      Returns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the raw API response as a JSON dictionary.\n",
      " |      \n",
      " |      Raises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n",
      " |  \n",
      " |  recognize_google_cloud(self, audio_data, credentials_json=None, language='en-US', preferred_phrases=None, show_all=False)\n",
      " |      Performs speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Google Cloud Speech API.\n",
      " |      \n",
      " |      This function requires a Google Cloud Platform account; see the `Google Cloud Speech API Quickstart <https://cloud.google.com/speech/docs/getting-started>`__ for details and instructions. Basically, create a project, enable billing for the project, enable the Google Cloud Speech API for the project, and set up Service Account Key credentials for the project. The result is a JSON file containing the API credentials. The text content of this JSON file is specified by ``credentials_json``. If not specified, the library will try to automatically `find the default API credentials JSON file <https://developers.google.com/identity/protocols/application-default-credentials>`__.\n",
      " |      \n",
      " |      The recognition language is determined by ``language``, which is a BCP-47 language tag like ``\"en-US\"`` (US English). A list of supported language tags can be found in the `Google Cloud Speech API documentation <https://cloud.google.com/speech/docs/languages>`__.\n",
      " |      \n",
      " |      If ``preferred_phrases`` is an iterable of phrase strings, those given phrases will be more likely to be recognized over similar-sounding alternatives. This is useful for things like keyword/command recognition or adding new phrases that aren't in Google's vocabulary. Note that the API imposes certain `restrictions on the list of phrase strings <https://cloud.google.com/speech/limits#content>`__.\n",
      " |      \n",
      " |      Returns the most likely transcription if ``show_all`` is False (the default). Otherwise, returns the raw API response as a JSON dictionary.\n",
      " |      \n",
      " |      Raises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the credentials aren't valid, or if there is no Internet connection.\n",
      " |  \n",
      " |  recognize_houndify(self, audio_data, client_id, client_key, show_all=False)\n",
      " |      Performs speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Houndify API.\n",
      " |      \n",
      " |      The Houndify client ID and client key are specified by ``client_id`` and ``client_key``, respectively. Unfortunately, these are not available without `signing up for an account <https://www.houndify.com/signup>`__. Once logged into the `dashboard <https://www.houndify.com/dashboard>`__, you will want to select \"Register a new client\", and fill in the form as necessary. When at the \"Enable Domains\" page, enable the \"Speech To Text Only\" domain, and then select \"Save & Continue\".\n",
      " |      \n",
      " |      To get the client ID and client key for a Houndify client, go to the `dashboard <https://www.houndify.com/dashboard>`__ and select the client's \"View Details\" link. On the resulting page, the client ID and client key will be visible. Client IDs and client keys are both Base64-encoded strings.\n",
      " |      \n",
      " |      Currently, only English is supported as a recognition language.\n",
      " |      \n",
      " |      Returns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the raw API response as a JSON dictionary.\n",
      " |      \n",
      " |      Raises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n",
      " |  \n",
      " |  recognize_ibm(self, audio_data, username, password, language='en-US', show_all=False)\n",
      " |      Performs speech recognition on ``audio_data`` (an ``AudioData`` instance), using the IBM Speech to Text API.\n",
      " |      \n",
      " |      The IBM Speech to Text username and password are specified by ``username`` and ``password``, respectively. Unfortunately, these are not available without `signing up for an account <https://console.ng.bluemix.net/registration/>`__. Once logged into the Bluemix console, follow the instructions for `creating an IBM Watson service instance <https://www.ibm.com/watson/developercloud/doc/getting_started/gs-credentials.shtml>`__, where the Watson service is \"Speech To Text\". IBM Speech to Text usernames are strings of the form XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX, while passwords are mixed-case alphanumeric strings.\n",
      " |      \n",
      " |      The recognition language is determined by ``language``, an RFC5646 language tag with a dialect like ``\"en-US\"`` (US English) or ``\"zh-CN\"`` (Mandarin Chinese), defaulting to US English. The supported language values are listed under the ``model`` parameter of the `audio recognition API documentation <https://www.ibm.com/watson/developercloud/speech-to-text/api/v1/#sessionless_methods>`__, in the form ``LANGUAGE_BroadbandModel``, where ``LANGUAGE`` is the language value.\n",
      " |      \n",
      " |      Returns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the `raw API response <https://www.ibm.com/watson/developercloud/speech-to-text/api/v1/#sessionless_methods>`__ as a JSON dictionary.\n",
      " |      \n",
      " |      Raises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n",
      " |  \n",
      " |  recognize_sphinx(self, audio_data, language='en-US', keyword_entries=None, grammar=None, show_all=False)\n",
      " |      Performs speech recognition on ``audio_data`` (an ``AudioData`` instance), using CMU Sphinx.\n",
      " |      \n",
      " |      The recognition language is determined by ``language``, an RFC5646 language tag like ``\"en-US\"`` or ``\"en-GB\"``, defaulting to US English. Out of the box, only ``en-US`` is supported. See `Notes on using `PocketSphinx <https://github.com/Uberi/speech_recognition/blob/master/reference/pocketsphinx.rst>`__ for information about installing other languages. This document is also included under ``reference/pocketsphinx.rst``. The ``language`` parameter can also be a tuple of filesystem paths, of the form ``(acoustic_parameters_directory, language_model_file, phoneme_dictionary_file)`` - this allows you to load arbitrary Sphinx models.\n",
      " |      \n",
      " |      If specified, the keywords to search for are determined by ``keyword_entries``, an iterable of tuples of the form ``(keyword, sensitivity)``, where ``keyword`` is a phrase, and ``sensitivity`` is how sensitive to this phrase the recognizer should be, on a scale of 0 (very insensitive, more false negatives) to 1 (very sensitive, more false positives) inclusive. If not specified or ``None``, no keywords are used and Sphinx will simply transcribe whatever words it recognizes. Specifying ``keyword_entries`` is more accurate than just looking for those same keywords in non-keyword-based transcriptions, because Sphinx knows specifically what sounds to look for.\n",
      " |      \n",
      " |      Sphinx can also handle FSG or JSGF grammars. The parameter ``grammar`` expects a path to the grammar file. Note that if a JSGF grammar is passed, an FSG grammar will be created at the same location to speed up execution in the next run. If ``keyword_entries`` are passed, content of ``grammar`` will be ignored.\n",
      " |      \n",
      " |      Returns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the Sphinx ``pocketsphinx.pocketsphinx.Decoder`` object resulting from the recognition.\n",
      " |      \n",
      " |      Raises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if there are any issues with the Sphinx installation.\n",
      " |  \n",
      " |  recognize_wit(self, audio_data, key, show_all=False)\n",
      " |      Performs speech recognition on ``audio_data`` (an ``AudioData`` instance), using the Wit.ai API.\n",
      " |      \n",
      " |      The Wit.ai API key is specified by ``key``. Unfortunately, these are not available without `signing up for an account <https://wit.ai/>`__ and creating an app. You will need to add at least one intent to the app before you can see the API key, though the actual intent settings don't matter.\n",
      " |      \n",
      " |      To get the API key for a Wit.ai app, go to the app's overview page, go to the section titled \"Make an API request\", and look for something along the lines of ``Authorization: Bearer XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX``; ``XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX`` is the API key. Wit.ai API keys are 32-character uppercase alphanumeric strings.\n",
      " |      \n",
      " |      The recognition language is configured in the Wit.ai app settings.\n",
      " |      \n",
      " |      Returns the most likely transcription if ``show_all`` is false (the default). Otherwise, returns the `raw API response <https://wit.ai/docs/http/20141022#get-intent-via-text-link>`__ as a JSON dictionary.\n",
      " |      \n",
      " |      Raises a ``speech_recognition.UnknownValueError`` exception if the speech is unintelligible. Raises a ``speech_recognition.RequestError`` exception if the speech recognition operation failed, if the key isn't valid, or if there is no internet connection.\n",
      " |  \n",
      " |  record(self, source, duration=None, offset=None)\n",
      " |      Records up to ``duration`` seconds of audio from ``source`` (an ``AudioSource`` instance) starting at ``offset`` (or at the beginning if not specified) into an ``AudioData`` instance, which it returns.\n",
      " |      \n",
      " |      If ``duration`` is not specified, then it will record until there is no more audio input.\n",
      " |  \n",
      " |  snowboy_wait_for_hot_word(self, snowboy_location, snowboy_hot_word_files, source, timeout=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  recognize_api(audio_data, client_access_token, language='en', session_id=None, show_all=False) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from AudioSource:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from AudioSource:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463df99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
